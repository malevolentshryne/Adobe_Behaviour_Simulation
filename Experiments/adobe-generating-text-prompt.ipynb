{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9569942,"sourceType":"datasetVersion","datasetId":5833129}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom PIL import Image, UnidentifiedImageError\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nimport requests\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torchvision import transforms\nfrom tqdm.notebook import tqdm_notebook as tqdm\nfrom io import BytesIO","metadata":{"execution":{"iopub.status.busy":"2024-10-15T12:08:51.832781Z","iopub.execute_input":"2024-10-15T12:08:51.833192Z","iopub.status.idle":"2024-10-15T12:08:58.030532Z","shell.execute_reply.started":"2024-10-15T12:08:51.833152Z","shell.execute_reply":"2024-10-15T12:08:58.029689Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.18 (you have 1.4.17). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-10-15T12:08:59.945042Z","iopub.execute_input":"2024-10-15T12:08:59.945598Z","iopub.status.idle":"2024-10-15T12:08:59.981757Z","shell.execute_reply.started":"2024-10-15T12:08:59.945562Z","shell.execute_reply":"2024-10-15T12:08:59.980764Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import BlipProcessor, BlipForConditionalGeneration\n\nmodel_name = \"Salesforce/blip-image-captioning-base\"\nprocessor = BlipProcessor.from_pretrained(model_name)\nmodel = BlipForConditionalGeneration.from_pretrained(model_name)\n\nmodel.to(device)\n\ndef generate_prompt_from_image(image_path):\n    if image_path.startswith(\"http\"):\n        image = Image.open(requests.get(image_path, stream=True).raw)\n    else:\n        image = Image.open(image_path)\n    \n    inputs = processor(images=image, return_tensors=\"pt\")\n    inputs.to(device)\n\n    # Generate the prompt\n    prompt_ids = model.generate(**inputs)\n    \n    # Decode the generated ids to text\n    prompt = processor.decode(prompt_ids[0], skip_special_tokens=True)\n    \n    return prompt\n\n# Example usage with a local image\nimage_path = \"path_to_your_image.jpg\"\nprompt = generate_prompt_from_image(image_path)\nprint(f\"Generated prompt: {prompt}\")","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7141390,"sourceType":"datasetVersion","datasetId":4121842},{"sourceId":9607552,"sourceType":"datasetVersion","datasetId":5862032}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\ntrain_data = pd.read_excel('/kaggle/input/adobe-nlp/behaviour_simulation_train.xlsx',index_col='id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T12:29:08.588918Z","iopub.execute_input":"2024-10-12T12:29:08.589562Z","iopub.status.idle":"2024-10-12T12:30:19.902701Z","shell.execute_reply.started":"2024-10-12T12:29:08.589515Z","shell.execute_reply":"2024-10-12T12:30:19.901677Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\nfnet = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n\nclass embeddings(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fnet = fnet\n        self.fc = nn.Sequential(nn.Linear(1000,256),nn.ReLU(),nn.Linear(256,1))\n        for param in self.fnet.parameters():\n            param.requires_grad=False\n        for param in self.fc.parameters():\n            param.requires_grad=True\n    def forward(self,x):\n        x = self.fnet(x)\n        return self.fc(x)\neffnet = embeddings()\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T12:30:42.960982Z","iopub.execute_input":"2024-10-12T12:30:42.961374Z","iopub.status.idle":"2024-10-12T12:30:43.365922Z","shell.execute_reply.started":"2024-10-12T12:30:42.961333Z","shell.execute_reply":"2024-10-12T12:30:43.365120Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n100%|██████████| 20.5M/20.5M [00:00<00:00, 165MB/s]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"state_dict_load = torch.load('/kaggle/input/effnet-trained/effnet-full-trained.pth')\neffnet.state_dict = state_dict_load\nfor param in effnet.parameters():\n    param.requires_grad=False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T12:33:50.793541Z","iopub.execute_input":"2024-10-12T12:33:50.794335Z","iopub.status.idle":"2024-10-12T12:33:50.884020Z","shell.execute_reply.started":"2024-10-12T12:33:50.794296Z","shell.execute_reply":"2024-10-12T12:33:50.883060Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"effnet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T12:34:17.137506Z","iopub.execute_input":"2024-10-12T12:34:17.138140Z","iopub.status.idle":"2024-10-12T12:34:17.151695Z","shell.execute_reply.started":"2024-10-12T12:34:17.138098Z","shell.execute_reply":"2024-10-12T12:34:17.150662Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"embeddings(\n  (fnet): EfficientNet(\n    (features): Sequential(\n      (0): Conv2dNormActivation(\n        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): SiLU(inplace=True)\n      )\n      (1): Sequential(\n        (0): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (2): Conv2dNormActivation(\n              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n        )\n      )\n      (2): Sequential(\n        (0): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n        )\n        (1): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n        )\n      )\n      (3): Sequential(\n        (0): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n        )\n        (1): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n        )\n      )\n      (4): Sequential(\n        (0): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n        )\n        (1): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n        )\n        (2): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n        )\n      )\n      (5): Sequential(\n        (0): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n        )\n        (1): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n        )\n        (2): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n        )\n      )\n      (6): Sequential(\n        (0): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n        )\n        (1): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n        )\n        (2): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n        )\n        (3): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n        )\n      )\n      (7): Sequential(\n        (0): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n        )\n      )\n      (8): Conv2dNormActivation(\n        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): SiLU(inplace=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=1)\n    (classifier): Sequential(\n      (0): Dropout(p=0.2, inplace=True)\n      (1): Linear(in_features=1280, out_features=1000, bias=True)\n    )\n  )\n  (fc): Sequential(\n    (0): Linear(in_features=1000, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=256, out_features=1, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport requests\nimport os\nimport gc\nfrom tqdm import tqdm\nfrom urllib.parse import urlparse\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\nfrom multiprocessing import Pool\nfrom transformers import AutoTokenizer, AutoModel\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef get_embedding_model():\n    l = list(effnet.children())[0]\n    l2 = list(effnet.children())[1][0]\n    combined_layers = nn.Sequential(l, l2)\n    c = list(combined_layers)\n    model = nn.Sequential(*c) # removing last layer that was used for finetuning\n    # features = list(model.children())[:-1]\n    # embedding_size = 128\n    # features.extend([\n    #     torch.nn.AdaptiveAvgPool2d(1),\n    #     torch.nn.Flatten(),\n    #     torch.nn.Linear(1280, embedding_size)\n    # ])\n    # embedding_model = torch.nn.Sequential(*features).to(device)\n    # embedding_model.eval()\n    model.to(device)\n    model.eval()\n    return model\n\ntransform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndef extract_url(media_string):\n    if pd.isna(media_string):\n        return None\n    if 'Photo' in media_string:\n        return media_string.split(\"previewUrl='\")[1].split(\"'\")[0]\n    else:\n        return media_string.split(\"thumbnailUrl='\")[1].split(\"'\")[0]\n    return None\n\ndef download_media(url, save_dir):\n    try:\n        response = requests.get(url, stream=True, timeout=10)\n        if response.status_code == 200:\n            file_name = os.path.basename(urlparse(url).path)\n            file_path = os.path.join(save_dir, file_name)\n            with open(file_path, 'wb') as f:\n                for chunk in response.iter_content(1024):\n                    f.write(chunk)\n            return file_path\n        else:\n            return None\n    except requests.RequestException:\n        return None\n\ndef download_media_parallel(urls, save_dir):\n    with Pool() as pool:\n        return pool.starmap(download_media, [(url, save_dir) for url in urls if url])\n\ndef get_batch_embeddings(img_paths, model, batch_size=32):\n    embeddings = []\n    for i in range(0, len(img_paths), batch_size):\n        batch_paths = img_paths[i:i+batch_size]\n        batch_imgs = [Image.open(path, mode='r', formats=['JPEG', 'PNG']).convert('RGB') for path in batch_paths if isinstance(path, str) and os.path.exists(path)]\n        if batch_imgs:\n            batch_tensors = torch.stack([transform(img) for img in batch_imgs]).to(device)\n            with torch.no_grad():\n                batch_embeddings = model(batch_tensors).cpu().numpy()\n            embeddings.extend(batch_embeddings)\n    return embeddings\n\ndef get_bertweet_embeddings(texts, model, tokenizer, max_length=128, batch_size=32):\n    embeddings = []\n    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating BERTweet embeddings\"):\n        batch_texts = texts[i:i+batch_size]\n        inputs = tokenizer(batch_texts, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=\"max_length\")\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        with torch.no_grad():\n            outputs = model(**inputs)\n        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # CLS token\n        embeddings.extend(batch_embeddings)\n        del inputs, outputs, batch_embeddings\n        torch.cuda.empty_cache()\n    return np.array(embeddings)\n\ndef preprocessing(df, save_dir='/kaggle/working/adobe/'):\n    # Date feature extraction\n    df['date'] = pd.to_datetime(df['date'])\n    df['month'] = df['date'].dt.month\n    df['is_weekend'] = (df['date'].dt.dayofweek>=5)*1\n    df['is_month_start'] = df['date'].dt.is_month_start*1\n    df['is_month_end'] = df['date'].dt.is_month_end*1\n    df['day'] = df['date'].dt.day\n    df = df.drop('date', axis=1)\n    \n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    \n    # Image processing\n    df['media_url'] = df['media'].apply(extract_url)\n    df['local_path'] = download_media_parallel(df['media_url'].dropna().tolist(), save_dir)\n    df['local_path'] = df['local_path'].apply(lambda x: x if x and os.path.exists(x) else None)\n    valid_paths = df['local_path'].dropna().tolist()\n    \n    embedding_model = get_embedding_model()\n    image_embeddings = get_batch_embeddings(valid_paths, embedding_model, batch_size=16)\n    image_embedding_df = pd.DataFrame(image_embeddings, columns=[f'image_embedding_{i}' for i in range(256)])\n    df = pd.concat([df.reset_index(drop=True), image_embedding_df], axis=1)\n    \n    # Text processing\n    df['combined_text'] = 'username : ' + df['username'] + ' '+ 'comapny : ' + df['inferred company']+'content : '+df['content'] \n    \n    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=True)\n    model = AutoModel.from_pretrained(\"vinai/bertweet-base\").to(device)\n    model.eval()\n    \n    bertweet_embeddings = get_bertweet_embeddings(df['combined_text'].tolist(), model, tokenizer, batch_size=32)\n    bertweet_embedding_df = pd.DataFrame(bertweet_embeddings, columns=[f'text_embedding_{i}' for i in range(768)])\n    df = pd.concat([df, bertweet_embedding_df], axis=1)\n    \n    # Drop unnecessary columns\n    df = df.drop(['media', 'media_url', 'local_path', 'content', 'username', 'inferred company', 'combined_text'], axis=1)\n    \n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T12:45:54.521460Z","iopub.execute_input":"2024-10-12T12:45:54.521859Z","iopub.status.idle":"2024-10-12T12:45:54.552771Z","shell.execute_reply.started":"2024-10-12T12:45:54.521822Z","shell.execute_reply":"2024-10-12T12:45:54.551945Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import warnings\nimport os\nwarnings.filterwarnings('ignore')\ntesting = train_data.copy()\ntill = 10000\ntesting = preprocessing(testing[:till])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T12:49:19.316455Z","iopub.execute_input":"2024-10-12T12:49:19.316843Z","iopub.status.idle":"2024-10-12T12:53:06.447712Z","shell.execute_reply.started":"2024-10-12T12:49:19.316807Z","shell.execute_reply":"2024-10-12T12:53:06.446637Z"}},"outputs":[{"name":"stderr","text":"Generating BERTweet embeddings: 100%|██████████| 313/313 [00:41<00:00,  7.51it/s]\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"len(list(testing.columns))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T12:53:06.450589Z","iopub.execute_input":"2024-10-12T12:53:06.450931Z","iopub.status.idle":"2024-10-12T12:53:06.457290Z","shell.execute_reply.started":"2024-10-12T12:53:06.450896Z","shell.execute_reply":"2024-10-12T12:53:06.456501Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"1030"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"testing.head()\ntesting.columns.get_loc('image_embedding_255')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T13:07:27.912116Z","iopub.execute_input":"2024-10-12T13:07:27.912523Z","iopub.status.idle":"2024-10-12T13:07:27.920075Z","shell.execute_reply.started":"2024-10-12T13:07:27.912485Z","shell.execute_reply":"2024-10-12T13:07:27.919122Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"261"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"# import warnings\n# import os\n# import pandas as pd\n# import numpy as np\n# from sklearn.model_selection import GroupKFold\n# from sklearn.metrics import mean_squared_error\n# from lightgbm import LGBMRegressor\n# from xgboost import XGBRegressor\n# from catboost import CatBoostRegressor\n# from sklearn.ensemble import RandomForestRegressor\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Dense, Dropout\n# from tensorflow.keras.optimizers import Adam\n# from sklearn.preprocessing import StandardScaler\n# import matplotlib.pyplot as plt\n\n# warnings.filterwarnings('ignore')\n\n# df = testing\n# # Select features\n# date_cols = ['month', 'day', 'is_month_start','is_month_end', 'is_weekend']\n# X = df[date_cols]\n# y = df['likes']\n# groups = train_data['inferred company'].iloc[:till]\n# X = X.fillna(0)\n\n# # Convert datetime columns to numeric\n# for col in X.select_dtypes(include=['datetime64']).columns:\n#     X[col] = X[col].astype(int) // 10**9  # Convert to Unix timestamp\n\n# # Initialize models\n# models = {\n#     'LightGBM': LGBMRegressor(random_state=42),\n#     'XGBoost': XGBRegressor(random_state=42),\n#     'CatBoost': CatBoostRegressor(random_state=42, verbose=0),\n#     'RandomForest': RandomForestRegressor(random_state=42)\n# }\n\n# # Function to create neural network\n# def create_nn_model(input_dim):\n#     model = Sequential([\n#         Dense(64, activation='relu', input_dim=input_dim),\n#         Dropout(0.2),\n#         Dense(32, activation='relu'),\n#         Dropout(0.2),\n#         Dense(16, activation='relu'),\n#         Dense(1)\n#     ])\n#     model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n#     return model\n\n# # Initialize GroupKFold\n# gkf = GroupKFold(n_splits=3)\n\n# # Dictionary to store RMSE scores\n# rmse_scores = {model_name: [] for model_name in models.keys()}\n# rmse_scores['Neural Network'] = []\n\n# # Perform GroupKFold cross-validation\n# for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups), 1):\n#     X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n#     y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    \n#     # Scale the features\n#     scaler = StandardScaler()\n#     X_train_scaled = scaler.fit_transform(X_train)\n#     X_val_scaled = scaler.transform(X_val)\n    \n#     print(f\"Fold {fold}\")\n    \n#     # Train and evaluate all models\n#     for model_name, model in models.items():\n#         model.fit(X_train_scaled, y_train)\n#         y_pred = model.predict(X_val_scaled)\n#         rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n#         rmse_scores[model_name].append(rmse)\n#         print(f\"{model_name} RMSE: {rmse:.4f}\")\n    \n#     # Train and evaluate Neural Network\n#     nn_model = create_nn_model(X_train_scaled.shape[1])\n#     nn_model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, verbose=0)\n#     y_pred = nn_model.predict(X_val_scaled).flatten()\n#     rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n#     rmse_scores['Neural Network'].append(rmse)\n#     print(f\"Neural Network RMSE: {rmse:.4f}\")\n    \n#     print()\n\n# # Calculate average RMSE for each model\n# avg_rmse = {model: np.mean(scores) for model, scores in rmse_scores.items()}\n\n# # Visualization\n# plt.figure(figsize=(12, 6))\n# plt.boxplot([rmse_scores[model] for model in rmse_scores.keys()], labels=list(rmse_scores.keys()))\n# plt.title('RMSE Comparison of Different Models')\n# plt.ylabel('RMSE')\n# plt.xticks(rotation=45)\n# plt.tight_layout()\n# plt.show()\n\n# # Bar plot for average RMSE\n# plt.figure(figsize=(10, 5))\n# plt.bar(avg_rmse.keys(), avg_rmse.values())\n# plt.title('Average RMSE of Different Models')\n# plt.ylabel('Average RMSE')\n# plt.xticks(rotation=45)\n# plt.tight_layout()\n# plt.show()\n\n# # Print average RMSE and best model\n# print(\"\\nAverage RMSE scores:\")\n# for model, avg_score in avg_rmse.items():\n#     print(f\"{model}: {avg_score:.4f}\")\n\n# best_model = min(avg_rmse, key=avg_rmse.get)\n# print(f\"\\nBest performing model: {best_model}\")","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-10-12T12:53:06.489090Z","iopub.execute_input":"2024-10-12T12:53:06.489374Z","iopub.status.idle":"2024-10-12T12:53:06.496988Z","shell.execute_reply.started":"2024-10-12T12:53:06.489343Z","shell.execute_reply":"2024-10-12T12:53:06.496118Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"testing.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T12:55:56.456479Z","iopub.execute_input":"2024-10-12T12:55:56.457610Z","iopub.status.idle":"2024-10-12T12:55:56.485142Z","shell.execute_reply.started":"2024-10-12T12:55:56.457552Z","shell.execute_reply":"2024-10-12T12:55:56.483834Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"   likes  month  is_weekend  is_month_start  is_month_end  day  \\\n0      1     12           1               0             0   12   \n1   2750      6           1               0             1   30   \n2     57      9           0               0             0   29   \n3    152     10           0               1             0    1   \n4     41     10           0               0             0   19   \n\n   image_embedding_0  image_embedding_1  image_embedding_2  image_embedding_3  \\\n0          -1.171910           1.012488           1.599925           0.228262   \n1          -0.433859          -0.373151           1.226917           0.926010   \n2           1.487716           1.093873          -0.522397          -1.325502   \n3           0.251258          -0.237406          -0.896643           0.817894   \n4          -0.935458           1.329709          -0.460308          -0.255327   \n\n   ...  text_embedding_758  text_embedding_759  text_embedding_760  \\\n0  ...            0.109581            0.048969           -0.003208   \n1  ...            0.183375            0.051587            0.034564   \n2  ...            0.133224            0.045438           -0.021889   \n3  ...            0.082127            0.030091            0.119243   \n4  ...            0.254545           -0.017587            0.089886   \n\n   text_embedding_761  text_embedding_762  text_embedding_763  \\\n0            0.041433            0.068599           -0.016013   \n1           -0.073746            0.265690           -0.143621   \n2            0.087877           -0.011531           -0.087014   \n3           -0.093048            0.225192           -0.028532   \n4            0.100898            0.054707           -0.076007   \n\n   text_embedding_764  text_embedding_765  text_embedding_766  \\\n0           -0.042945           -0.221615           -0.131004   \n1            0.122211           -0.104689           -0.242918   \n2            0.096057           -0.116086           -0.164340   \n3            0.088843            0.033127           -0.139336   \n4           -0.068115           -0.238209           -0.111949   \n\n   text_embedding_767  \n0            0.017417  \n1           -0.003308  \n2           -0.053712  \n3           -0.252549  \n4           -0.041877  \n\n[5 rows x 1030 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>likes</th>\n      <th>month</th>\n      <th>is_weekend</th>\n      <th>is_month_start</th>\n      <th>is_month_end</th>\n      <th>day</th>\n      <th>image_embedding_0</th>\n      <th>image_embedding_1</th>\n      <th>image_embedding_2</th>\n      <th>image_embedding_3</th>\n      <th>...</th>\n      <th>text_embedding_758</th>\n      <th>text_embedding_759</th>\n      <th>text_embedding_760</th>\n      <th>text_embedding_761</th>\n      <th>text_embedding_762</th>\n      <th>text_embedding_763</th>\n      <th>text_embedding_764</th>\n      <th>text_embedding_765</th>\n      <th>text_embedding_766</th>\n      <th>text_embedding_767</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>12</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12</td>\n      <td>-1.171910</td>\n      <td>1.012488</td>\n      <td>1.599925</td>\n      <td>0.228262</td>\n      <td>...</td>\n      <td>0.109581</td>\n      <td>0.048969</td>\n      <td>-0.003208</td>\n      <td>0.041433</td>\n      <td>0.068599</td>\n      <td>-0.016013</td>\n      <td>-0.042945</td>\n      <td>-0.221615</td>\n      <td>-0.131004</td>\n      <td>0.017417</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2750</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>30</td>\n      <td>-0.433859</td>\n      <td>-0.373151</td>\n      <td>1.226917</td>\n      <td>0.926010</td>\n      <td>...</td>\n      <td>0.183375</td>\n      <td>0.051587</td>\n      <td>0.034564</td>\n      <td>-0.073746</td>\n      <td>0.265690</td>\n      <td>-0.143621</td>\n      <td>0.122211</td>\n      <td>-0.104689</td>\n      <td>-0.242918</td>\n      <td>-0.003308</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>57</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>29</td>\n      <td>1.487716</td>\n      <td>1.093873</td>\n      <td>-0.522397</td>\n      <td>-1.325502</td>\n      <td>...</td>\n      <td>0.133224</td>\n      <td>0.045438</td>\n      <td>-0.021889</td>\n      <td>0.087877</td>\n      <td>-0.011531</td>\n      <td>-0.087014</td>\n      <td>0.096057</td>\n      <td>-0.116086</td>\n      <td>-0.164340</td>\n      <td>-0.053712</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>152</td>\n      <td>10</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.251258</td>\n      <td>-0.237406</td>\n      <td>-0.896643</td>\n      <td>0.817894</td>\n      <td>...</td>\n      <td>0.082127</td>\n      <td>0.030091</td>\n      <td>0.119243</td>\n      <td>-0.093048</td>\n      <td>0.225192</td>\n      <td>-0.028532</td>\n      <td>0.088843</td>\n      <td>0.033127</td>\n      <td>-0.139336</td>\n      <td>-0.252549</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>41</td>\n      <td>10</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>19</td>\n      <td>-0.935458</td>\n      <td>1.329709</td>\n      <td>-0.460308</td>\n      <td>-0.255327</td>\n      <td>...</td>\n      <td>0.254545</td>\n      <td>-0.017587</td>\n      <td>0.089886</td>\n      <td>0.100898</td>\n      <td>0.054707</td>\n      <td>-0.076007</td>\n      <td>-0.068115</td>\n      <td>-0.238209</td>\n      <td>-0.111949</td>\n      <td>-0.041877</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1030 columns</p>\n</div>"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"import warnings\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split  # Import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings('ignore')\n\ndf = testing\nX = df[list(testing.columns)[1:]]\ny = df['likes']\nX = X.fillna(0)\n# Convert datetime columns to numeric\nfor col in X.select_dtypes(include=['datetime64']).columns:\n    X[col] = X[col].astype(int) // 10**9  # Convert to Unix timestamp\n\n# Initialize models\nmodels = {\n    'LightGBM': LGBMRegressor(random_state=42,n_estimators=1000),\n    'XGBoost': XGBRegressor(random_state=42),\n    'CatBoost': CatBoostRegressor(random_state=42, verbose=0),\n    'RandomForest': RandomForestRegressor(random_state=42)\n}\n\n# Function to create neural network\ndef create_nn_model(input_dim):\n    model = Sequential([\n        Dense(64, activation='relu', input_dim=input_dim),\n        Dropout(0.2),\n        Dense(32, activation='relu'),\n        Dropout(0.2),\n        Dense(16, activation='relu'),\n        Dense(1)\n    ])\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n    return model\n\n# Train-test split instead of GroupKFold\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42,shuffle=False)\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\n\n# Dictionary to store RMSE scores\nrmse_scores = {model_name: [] for model_name in models.keys()}\nrmse_scores['Neural Network'] = []\n\n# Train and evaluate all models\nprint(\"Training and evaluating models...\")\nfor model_name, model in models.items():\n    model.fit(X_train_scaled, y_train)\n    y_pred = model.predict(X_val_scaled)\n    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n    rmse_scores[model_name].append(rmse)\n    print(f\"{model_name} RMSE: {rmse:.4f}\")\n\n# Train and evaluate Neural Network\nnn_model = create_nn_model(X_train_scaled.shape[1])\nnn_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=0)\ny_pred = nn_model.predict(X_val_scaled).flatten()\nrmse = np.sqrt(mean_squared_error(y_val, y_pred))\nrmse_scores['Neural Network'].append(rmse)\nprint(f\"Neural Network RMSE: {rmse:.4f}\")\n\n# Calculate average RMSE for each model\navg_rmse = {model: np.mean(scores) for model, scores in rmse_scores.items()}\n\n# Visualization\nplt.figure(figsize=(12, 6))\nplt.boxplot([rmse_scores[model] for model in rmse_scores.keys()], labels=list(rmse_scores.keys()))\nplt.title('RMSE Comparison of Different Models')\nplt.ylabel('RMSE')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Bar plot for average RMSE\nplt.figure(figsize=(10, 5))\nplt.bar(avg_rmse.keys(), avg_rmse.values())\nplt.title('Average RMSE of Different Models')\nplt.ylabel('Average RMSE')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Print average RMSE and best model\nprint(\"\\nAverage RMSE scores:\")\nfor model, avg_score in avg_rmse.items():\n    print(f\"{model}: {avg_score:.4f}\")\n\nbest_model = min(avg_rmse, key=avg_rmse.get)\nprint(f\"\\nBest performing model: {best_model}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T14:10:08.165165Z","iopub.execute_input":"2024-10-12T14:10:08.165930Z"}},"outputs":[{"name":"stdout","text":"Training and evaluating models...\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097888 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261174\n[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 1029\n[LightGBM] [Info] Start training from score 741.199750\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
